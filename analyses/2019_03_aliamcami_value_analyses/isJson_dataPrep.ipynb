{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/overscripted/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import tldextract\n",
    "import hashlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sub samples and new samples with new columns/data will be saved under the \"DIR\" directory to keep things organized. \n",
    "As such, the function \"save_parquet\" and \"read_parquet\" adds this directory to every parquet name, and I'm using this functions instead of dd.read_parquet/dd.to_parquet direct to ensure the same read and write settings across the notebook. \n",
    "\n",
    "NOTE: each section adds its name to the 'FILE_NAME' and saves the new parquet with this name. Because of it, you can run the sections at any order you desire to have the output you need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing client / distributed\n",
    "# client = Client()\n",
    "# client\n",
    "\n",
    "#Create folder to save/read new data\n",
    "DIR = 'sample0_prep/'\n",
    "FILE_NAME = 's0'\n",
    "\n",
    "if not os.path.exists(DIR):\n",
    "    os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no \"recalculate_partition\" is passed on, it will not recalculate the partitions. It is not mandatory, but good if you are significantly reducing the size of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a DF to a parquet\n",
    "def save_parquet(df, name, recalculate_partition=False):\n",
    "    with ProgressBar():\n",
    "        #DF.REPARTITION copyed from: https://stackoverflow.com/questions/44657631/strategy-for-partitioning-dask-dataframes-efficiently\n",
    "        if recalculate_partition:\n",
    "            n = 1+df.memory_usage(deep=True).sum().compute() // (1000 * 1000 * 100)\n",
    "            print(\"Npartition: \", n)\n",
    "            df.repartition(npartitions= n).to_parquet(DIR + name + '.parquet', engine=\"pyarrow\")\n",
    "        else:\n",
    "            df.to_parquet(DIR + name + '.parquet', engine=\"pyarrow\")\n",
    "        \n",
    "        \n",
    "def read_parquet(name):\n",
    "    return dd.read_parquet(DIR + name + '.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Using 10% sample and self produced samples\n",
    " - 10% sample has 11292867 rows\n",
    " - Filtered by value_len > df.mean() has 499805 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['value_1000', 'value', 'value_len', 'symbol', 'script_url', 'location',\n",
       "       'operation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Original sample \n",
    "df = dd.read_parquet('sample_0.parquet', \n",
    "                     engine='pyarrow', \n",
    "                     columns=['value_1000', 'value', 'value_len', 'symbol', 'script_url', 'location', 'operation'])\n",
    "\n",
    "# df.astype({'value_1000': str, 'value': str,'value_len': int,'symbol': int,'script_url': str})\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF overview\n",
    "Some overview about the sample: \n",
    "- Mean: 1356.97,\n",
    "- Min: 0,\n",
    "- Max: 4496861\n",
    "- Std: 26310.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 58.7s\n",
      "1356.9776628910975 0 4496861 26310.62140481331 11292867\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    df_mean = df['value_len'].mean()\n",
    "    df_min = df['value_len'].min()\n",
    "    df_max = df['value_len'].max()\n",
    "    df_std = df['value_len'].std()\n",
    "    df_len = df['value_len'].count()\n",
    "    (df_mean, df_min, df_max, df_std, df_len) = dd.compute(df_mean, df_min, df_max, df_std, df_len);\n",
    "    print(df_mean, df_min, df_max, df_std, df_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Column: Domains\n",
    "The following code is copyed from this same project: ~/analyses/hello_world.ipynb\n",
    "\n",
    "It uses the data saved from the last section\n",
    "This section is dedicated to extract the domain of the columns \"location\" and \"script_url\" and add it as new columns \"location_domain\" and \"script_domain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook name:  s0_domains\n"
     ]
    }
   ],
   "source": [
    "FILE_NAME += '_domains'\n",
    "print('Notebook name: ', FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(url):\n",
    "    \"\"\"Use tldextract to return the base domain from a url\"\"\"\n",
    "    try:\n",
    "        extracted = tldextract.extract(url)\n",
    "        return '{}.{}'.format(extracted.domain, extracted.suffix)\n",
    "    except Exception as e:\n",
    "        return 'ERROR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype({'value_1000': str, 'value': str,'value_len': int,'symbol': int,'script_url': str, 'location': str})\n",
    "df['location_domain'] = df.location.apply(extract_domain, meta='O')\n",
    "df['script_domain'] = df.script_url.apply(extract_domain, meta='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  6min 23.0s\n"
     ]
    }
   ],
   "source": [
    "#save\n",
    "save_parquet(df=df, name=FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_domain</th>\n",
       "      <th>location</th>\n",
       "      <th>script_domain</th>\n",
       "      <th>script_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vk.com</td>\n",
       "      <td>https://vk.com/widget_comments.php?app=2297596...</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>https://vk.com/js/api/xdm.js?1449919642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vk.com</td>\n",
       "      <td>https://vk.com/widget_comments.php?app=2297596...</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>https://vk.com/js/api/xdm.js?1449919642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vk.com</td>\n",
       "      <td>https://vk.com/widget_comments.php?app=2297596...</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>https://vk.com/js/al/aes_light.js?592436914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baidu.com</td>\n",
       "      <td>https://pos.baidu.com/s?hei=70&amp;wid=670&amp;di=u313...</td>\n",
       "      <td>baidustatic.com</td>\n",
       "      <td>https://cpro.baidustatic.com/cpro/ui/noexpire/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>serienjunkies.org</td>\n",
       "      <td>http://serienjunkies.org/smilf/smilf-season-1-...</td>\n",
       "      <td>google.com</td>\n",
       "      <td>https://apis.google.com/js/plusone.js?_=151338...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_domain                                           location  \\\n",
       "0             vk.com  https://vk.com/widget_comments.php?app=2297596...   \n",
       "1             vk.com  https://vk.com/widget_comments.php?app=2297596...   \n",
       "2             vk.com  https://vk.com/widget_comments.php?app=2297596...   \n",
       "3          baidu.com  https://pos.baidu.com/s?hei=70&wid=670&di=u313...   \n",
       "4  serienjunkies.org  http://serienjunkies.org/smilf/smilf-season-1-...   \n",
       "\n",
       "     script_domain                                         script_url  \n",
       "0           vk.com            https://vk.com/js/api/xdm.js?1449919642  \n",
       "1           vk.com            https://vk.com/js/api/xdm.js?1449919642  \n",
       "2           vk.com        https://vk.com/js/al/aes_light.js?592436914  \n",
       "3  baidustatic.com  https://cpro.baidustatic.com/cpro/ui/noexpire/...  \n",
       "4       google.com  https://apis.google.com/js/plusone.js?_=151338...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read\n",
    "df = read_parquet(FILE_NAME)\n",
    "df[['location_domain',  'location', 'script_domain', 'script_url']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Column:  is_json\n",
    "\n",
    "After manual initial analysis I have think that the huge values are json structured, to validate that I included an new column that is a boolean value with the validation of json\n",
    "\n",
    "After simple validation of value is a json or not, boolean value will be saved on a new column named \"is_json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook name:  s0_domains_isjson\n"
     ]
    }
   ],
   "source": [
    "FILE_NAME += '_isjson'\n",
    "print('Notebook name: ', FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_json(myjson):\n",
    "    if (myjson == '{}'):\n",
    "        #would be counted as valid, but its an empty json\n",
    "        return False\n",
    "    try:\n",
    "        #Eliminate false positives\n",
    "        return (type(json.loads(myjson)) == dict)\n",
    "    except ValueError as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_json'] = df['value'].apply(is_json, meta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  4min 21.6s\n"
     ]
    }
   ],
   "source": [
    "#save\n",
    "save_parquet(df=df, name=FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_1000</th>\n",
       "      <th>is_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fXDcab74</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fXDcab74</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_ga=GA1.2.1529583939.1513387469; _gid=GA1.2.17...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          value_1000  is_json\n",
       "0                                           fXDcab74    False\n",
       "1                                           fXDcab74    False\n",
       "2  Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko...    False\n",
       "3  Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko...    False\n",
       "4  _ga=GA1.2.1529583939.1513387469; _gid=GA1.2.17...    False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read\n",
    "df = read_parquet(FILE_NAME)\n",
    "df[['value_1000', 'is_json']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Column:  value_md5\n",
    "Include new columns called \"value_md5\" that is the md5 of value column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook name:  s0_domains_isjson_md5\n"
     ]
    }
   ],
   "source": [
    "FILE_NAME += '_md5'\n",
    "print('Notebook name: ', FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5(value):\n",
    "    return hashlib.md5(value.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['value_md5'] = df['value'].apply(md5, meta='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2min 45.9s\n"
     ]
    }
   ],
   "source": [
    "#save\n",
    "save_parquet(df=df, name=FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_1000</th>\n",
       "      <th>value_md5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fXDcab74</td>\n",
       "      <td>7df64196939a8b6ff11482ed6df4b25a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fXDcab74</td>\n",
       "      <td>7df64196939a8b6ff11482ed6df4b25a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko...</td>\n",
       "      <td>bc0aac3569031babbd73e069947a4b12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko...</td>\n",
       "      <td>bc0aac3569031babbd73e069947a4b12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_ga=GA1.2.1529583939.1513387469; _gid=GA1.2.17...</td>\n",
       "      <td>324dd29b8c6438bc700ac2d85e33f12d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          value_1000  \\\n",
       "0                                           fXDcab74   \n",
       "1                                           fXDcab74   \n",
       "2  Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko...   \n",
       "3  Mozilla/5.0 (X11; Linux x86_64; rv:52.0) Gecko...   \n",
       "4  _ga=GA1.2.1529583939.1513387469; _gid=GA1.2.17...   \n",
       "\n",
       "                          value_md5  \n",
       "0  7df64196939a8b6ff11482ed6df4b25a  \n",
       "1  7df64196939a8b6ff11482ed6df4b25a  \n",
       "2  bc0aac3569031babbd73e069947a4b12  \n",
       "3  bc0aac3569031babbd73e069947a4b12  \n",
       "4  324dd29b8c6438bc700ac2d85e33f12d  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read\n",
    "df = read_parquet(FILE_NAME)\n",
    "df[['value_1000', 'value_md5']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving other possible usefull filtered samples to future analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## value_len > df_mean\n",
    "1356 is the value_len mean\n",
    "\n",
    "To filter the data into something that is more interesting to this task I decided to only work with values that are at above the mean.\n",
    "\n",
    "All values above the mean count up to 499805 rows. That is just 4,42% of the whole sample, and a lot easier to work on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook name:  s0_domains_isjson_md5_above_mean\n"
     ]
    }
   ],
   "source": [
    "name = FILE_NAME + '_above_mean'\n",
    "print('Notebook name: ', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 50.5s\n",
      "Npartition:  245\n",
      "[########################################] | 100% Completed |  1min 38.3s\n"
     ]
    }
   ],
   "source": [
    "#Save\n",
    "save_parquet(df= df[df['value_len'] > df_mean], name= name, recalculate_partition=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['value_1000', 'value', 'value_len', 'symbol', 'script_url', 'location',\n",
       "       'operation', 'location_domain', 'script_domain', 'is_json',\n",
       "       'value_md5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read\n",
    "df = read_parquet(name)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to parquet containing only JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook name:  s0_domains_isjson_md5_JSON_ONLY\n"
     ]
    }
   ],
   "source": [
    "name = FILE_NAME + '_JSON_ONLY'\n",
    "print('Notebook name: ', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 28.9s\n",
      "Npartition:  233\n",
      "[########################################] | 100% Completed |  1min  5.0s\n"
     ]
    }
   ],
   "source": [
    "save_parquet(df=df[df['is_json'] == True], name=name, recalculate_partition=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_1000</th>\n",
       "      <th>is_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"im-settings\":\"{\\\"val\\\":{\\\"settings\\\":{\\\"Site...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"dueljs_channel_comm\":\"[{\\\"id\\\":4734405521279...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"dueljs_channel_comm\":\"[{\\\"id\\\":4734405521279...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          value_1000  is_json\n",
       "0  {\"im-settings\":\"{\\\"val\\\":{\\\"settings\\\":{\\\"Site...     True\n",
       "1  {\"APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c...     True\n",
       "2  {\"APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c...     True\n",
       "3  {\"dueljs_channel_comm\":\"[{\\\"id\\\":4734405521279...     True\n",
       "4  {\"dueljs_channel_comm\":\"[{\\\"id\\\":4734405521279...     True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read all_json_above_mean\n",
    "df = read_parquet(name)\n",
    "df[['value_1000', 'is_json']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add json keys and schema columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the top level keys, sort them and add as a list into another column named 'json_keys'\n",
    "Will be using \"https://github.com/rnd0101/json_schema_inferencer\" to guess the json schema and save it into another column called \"json_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook name:  s0_domains_isjson_md5_JSON_ONLY_schema_keys\n"
     ]
    }
   ],
   "source": [
    "name += '_schema_keys'\n",
    "print('Notebook name: ', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json_schema_inferencer.guess_json_schema import guess_schema\n",
    "\n",
    "def jsonSchema(myjson):\n",
    "    try:\n",
    "        dct = json.loads(myjson)\n",
    "        value = guess_schema(dct)\n",
    "        l = list(value['properties'])\n",
    "        l.sort()\n",
    "        return l\n",
    "    except ValueError as e:\n",
    "        return list()\n",
    "    \n",
    "def jsonKeys(myjson):\n",
    "    try:\n",
    "        dct = json.loads(myjson)\n",
    "        keys = list(dct.keys())\n",
    "        keys.sort()\n",
    "        return keys\n",
    "    except ValueError as e:\n",
    "        return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  4min 18.1s\n"
     ]
    }
   ],
   "source": [
    "df['json_keys'] = df.value.apply(jsonKeys, meta='O')\n",
    "df['json_schema'] = df.value.apply(jsonSchema, meta='O')\n",
    "save_parquet(df=df, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_1000</th>\n",
       "      <th>json_keys</th>\n",
       "      <th>json_schema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"im-settings\":\"{\\\"val\\\":{\\\"settings\\\":{\\\"Site...</td>\n",
       "      <td>[im-settings]</td>\n",
       "      <td>[im-settings]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c...</td>\n",
       "      <td>[APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c]</td>\n",
       "      <td>[APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c...</td>\n",
       "      <td>[APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c]</td>\n",
       "      <td>[APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"dueljs_channel_comm\":\"[{\\\"id\\\":4734405521279...</td>\n",
       "      <td>[LastSearch, LastSearch_e, dueljs_channel_comm...</td>\n",
       "      <td>[LastSearch, LastSearch_e, dueljs_channel_comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"dueljs_channel_comm\":\"[{\\\"id\\\":4734405521279...</td>\n",
       "      <td>[LastSearch, LastSearch_e, dueljs_channel_comm...</td>\n",
       "      <td>[LastSearch, LastSearch_e, dueljs_channel_comm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          value_1000  \\\n",
       "0  {\"im-settings\":\"{\\\"val\\\":{\\\"settings\\\":{\\\"Site...   \n",
       "1  {\"APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c...   \n",
       "2  {\"APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c...   \n",
       "3  {\"dueljs_channel_comm\":\"[{\\\"id\\\":4734405521279...   \n",
       "4  {\"dueljs_channel_comm\":\"[{\\\"id\\\":4734405521279...   \n",
       "\n",
       "                                           json_keys  \\\n",
       "0                                      [im-settings]   \n",
       "1     [APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c]   \n",
       "2     [APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c]   \n",
       "3  [LastSearch, LastSearch_e, dueljs_channel_comm...   \n",
       "4  [LastSearch, LastSearch_e, dueljs_channel_comm...   \n",
       "\n",
       "                                         json_schema  \n",
       "0                                      [im-settings]  \n",
       "1     [APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c]  \n",
       "2     [APLUS_S_CORE_0.17.12_20171214163401_2ee09a0c]  \n",
       "3  [LastSearch, LastSearch_e, dueljs_channel_comm...  \n",
       "4  [LastSearch, LastSearch_e, dueljs_channel_comm...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read \n",
    "df = read_parquet(name)\n",
    "df[['value_1000', 'json_keys', 'json_schema']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All NON json above the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook name:  s0_domains_isjson_md5_nonJSON_ONLY\n"
     ]
    }
   ],
   "source": [
    "name = FILE_NAME + '_nonJSON_ONLY'\n",
    "df = read_parquet(FILE_NAME)\n",
    "print('Notebook name: ', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 54.5s\n",
      "Npartition:  116\n",
      "[########################################] | 100% Completed |  1min 13.1s\n"
     ]
    }
   ],
   "source": [
    "save_parquet(df=df[df['is_json'] == False], name=name, recalculate_partition=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/overscripted/lib/python3.6/site-packages/dask/dataframe/core.py:4494: UserWarning: Insufficient elements for `head`. 5 elements requested, only 0 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(msg.format(n, len(r)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_1000</th>\n",
       "      <th>value</th>\n",
       "      <th>value_len</th>\n",
       "      <th>symbol</th>\n",
       "      <th>script_url</th>\n",
       "      <th>location</th>\n",
       "      <th>operation</th>\n",
       "      <th>location_domain</th>\n",
       "      <th>script_domain</th>\n",
       "      <th>is_json</th>\n",
       "      <th>value_md5</th>\n",
       "      <th>json_keys</th>\n",
       "      <th>json_schema</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [value_1000, value, value_len, symbol, script_url, location, operation, location_domain, script_domain, is_json, value_md5, json_keys, json_schema]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read \n",
    "df = read_parquet(name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
